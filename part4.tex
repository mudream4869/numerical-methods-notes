\section{Part4}

\subsection{Norms of Vector and Matrix}

A \textbf{norm} should satisfy three

\begin{align}
  \Vert A \Vert &\geq 0 \nonumber \\
  \Vert A \Vert + \Vert B \Vert &\geq \Vert A+B \Vert \nonumber \\
  \Vert \alpha A \Vert &= |\alpha| \Vert A \Vert \nonumber
\end{align}

For example, there are some norms for vector
\begin{enumerate}
  \item 2-norm of $\mathbf{R} ^ 2$ :
    $\Vert (x, y) \Vert = \sqrt{x^2 + y^2}$
  \item p-norm of $\mathbf{R} ^ n$ :
    $\Vert x \Vert = \left( \sum_{i=1}^{n} x_i^p \right)^\frac{1}{p}$
  \item max-norm of $\mathbf{R} ^ n$ :
    $\Vert x \Vert = \max_i |x_i|$ 
\end{enumerate}

Note that when $p < 1$, $p-norm$ is not a \textbf{norm}.

Actually we can define matrix norm to be a "long-vector" norm,
but it didn't preserve some special property of the matrix.

A matrix norm usually define as $\max_{\Vert v\Vert} \Vert Av \Vert$,
and the norm do have some great property in linear system.
Consider $Ax = b$, we slightly modify RHS and obtain
$A(x + \delta x) = b + \delta b$.

\begin{align}
    \Vert \delta x\Vert = \Vert A^{-1}b \Vert &\leq \Vert A^{-1} \Vert \Vert \delta b \Vert \nonumber \\
    \Vert b\Vert = \Vert Ax \Vert &\leq \Vert A \Vert \Vert x \Vert \nonumber
\end{align}

Combine and we can get

\[
  \frac{\Vert \delta x \Vert}{\Vert x \Vert} \leq \Vert A \Vert \Vert A^{-1} \Vert
                                                  \frac{\Vert \delta b \Vert}{\Vert b\Vert}
\]

Or we can slightly modify LHS :
$(A + \delta A)(x + \delta x) = b$.

\begin{align}
  &\delta x = -A^{-1}\delta A(x + \delta x) \nonumber \\
  &\Rightarrow \Vert x \Vert \leq
    \Vert A^{-1} \Vert \Vert \delta A \Vert \Vert x + \delta x \Vert \nonumber \\
  &\Rightarrow \frac{\Vert x \Vert}{\Vert x + \delta x \Vert} \leq
    \Vert A \Vert \Vert A^{-1} \Vert \frac{\Vert \delta A \Vert}{\Vert A \Vert} \nonumber
\end{align}

and $\Vert A \Vert \Vert A^{-1} \Vert$ call \textbf{condition} of $A$

\subsection{Iterative Method for Linear System}

Consider linear system in $\mathbf{R}^n$ $Ax = b$,
the Gaussian elimination should take $O(n^3)$ time. 
We will try to use iterative approach to solve linear system.
When we have a \textbf{solution},
we hope to calculate a more accurate \textbf{solution} by previous \textbf{solution}.
The time $O(iteration\ time \times iterate\ calculation)$ should be smaller than $O(n^3)$

\subsubsection{Jacobi and Gauss-Seidel Method}

Jacobi Method take $O(n^2)$ in every iteration, $O(n)$ for each dim in vector:

\newcommand{\diag}{\mathop{\mathrm{diag}}}

\[
  \diag(A)x_{i+1} = (\diag(A) - A)x_i
\]

For example, dim 3 should be :

\begin{align}
  (x_1)_{i+1} &= \frac{A_{12}(x_2)_i + A_{13}(x_3)_i}{A_{11}} \nonumber \\
  (x_2)_{i+1} &= \frac{A_{21}(x_1)_i + A_{23}(x_3)_i}{A_{22}} \nonumber \\
  (x_3)_{i+1} &= \frac{A_{31}(x_1)_i + A_{32}(x_2)_i}{A_{33}} \nonumber
\end{align}

Gauss-Seidel change the used previous vector to on-going updating solution.

\begin{align}
  (x_1)_{i+1} &= \frac{A_{12}(x_2)_i + A_{13}(x_3)_i}{A_{11}} \nonumber \\
  (x_2)_{i+1} &= \frac{A_{21}(x_1)_{i+1} + A_{23}(x_3)_i}{A_{22}} \nonumber \\
  (x_3)_{i+1} &= \frac{A_{31}(x_1)_{i+1} + A_{32}(x_2)_{i+1}}{A_{33}} \nonumber
\end{align}

Note that the iteration might diverge.

\subsubsection{Conjugate Gradient Method}

Now we approach the target by mininizing another
function $\min_x f(x) = \min_x \frac{1}{2}x^TAx - b^Tx$.
If we apply gradient descent, we could find out that
the descent direction should be $-\nabla f(x)$.
Then we mininize length parameter $\alpha$ in $x - x_k = - \alpha \nabla f(x)$.
This is a simple one-variable two-dimension optimization.

\[
  \alpha = \frac{r^T(b-Ax)}{r^TAr}
\]
